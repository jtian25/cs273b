{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = pd.read_csv('peaks_all.csv')\n",
    "# print(data.head)\n",
    "# print(data.shape)\n",
    "# row_0 = np.array(data.columns)[1:]\n",
    "# print(len(row_0))\n",
    "# data_t = data.T\n",
    "# print(data_t.shape)\n",
    "# data_t = data_t.to_numpy()\n",
    "# x = data_t[1:,:]\n",
    "# with open('x.pickle', 'wb') as f:\n",
    "#     pickle.dump(x,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('metadata_seurat_onelayer.csv')\n",
    "y = np.array(meta_data.loc[:,\"Celltype\"])\n",
    "d_y = defaultdict(int)\n",
    "\n",
    "new_y = []\n",
    "idxs_to_keep = []\n",
    "\n",
    "for i,label in enumerate(y):\n",
    "    d_y[label] += 1\n",
    "    if d_y[label] < 500:\n",
    "        new_y.append(label)\n",
    "        idxs_to_keep.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x.pickle','rb') as f:\n",
    "    imported_x = torch.Tensor(pickle.load(f).astype(float))\n",
    "print(imported_x.shape)\n",
    "\n",
    "x = imported_x[idxs_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set = set(y)\n",
    "print(len(y_set))\n",
    "labels_dict = defaultdict(int)\n",
    "label_num = 0\n",
    "for item in y_set:\n",
    "    labels_dict[item] = label_num\n",
    "    label_num += 1\n",
    "\n",
    "labels = []\n",
    "for item in y:\n",
    "    labels.append(labels_dict[item])\n",
    "labels = np.array(labels)\n",
    "labels = torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_labels_dict = defaultdict(str)\n",
    "for key,value in labels_dict.items():\n",
    "    inverted_labels_dict[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_d = defaultdict(int)\n",
    "\n",
    "for label in y:\n",
    "    totals_d[label] += 1\n",
    "\n",
    "plt.bar(*zip(*totals_d.items()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL_DATAPOINTS = 14322\n",
    "# NUM_TRAIN = 12890\n",
    "# NUM_VAL = 716\n",
    "# NUM_TEST = 716\n",
    "\n",
    "TOTAL_DATAPOINTS = 6610\n",
    "NUM_TRAIN = 5949\n",
    "NUM_VAL = 331\n",
    "NUM_TEST = 330\n",
    "\n",
    "\n",
    "labels = torch.Tensor(labels)\n",
    "dataset = TensorDataset(x,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will automatically randomly split your dataset but you could also implement the split yourself\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [NUM_TRAIN, NUM_TEST, NUM_VAL])\n",
    "\n",
    "loader_train = DataLoader(train_set, batch_size=64)\n",
    "loader_val = DataLoader(val_set, batch_size=64)\n",
    "\n",
    "loader_test = DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1, start_epoch=0):\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    loss_list = []\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    for e in range(epochs):\n",
    "        print(f\"epoch: {e+1}/{epochs}\")\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            #x = x.unsqueeze(dim = 1)\n",
    "         \n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "        \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                loss_list.append(loss.item())\n",
    "                print(\"Train Accuracy:\")\n",
    "                train_acc = check_accuracy(loader_train, model)\n",
    "                print(\"Validation Accuracy:\")\n",
    "                val_acc = check_accuracy(loader_val, model)\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "                np.savetxt(\"train.txt\", np.array(train_acc_list))\n",
    "                np.savetxt(\"val_acc.txt\", np.array(val_acc_list))\n",
    "                np.savetxt(\"losslist.txt\", np.array(loss_list))\n",
    "                print()\n",
    "\n",
    "        if e % 20 == 0:\n",
    "            torch.save(model.state_dict(), f'model_weights_orig_new_data_{e}.pth')\n",
    "            print(\"model saved\")\n",
    "                \n",
    "\n",
    "    return loss_list, train_acc_list, val_acc_list\n",
    "\n",
    "def train_conv(model, optimizer, epochs=1, start_epoch=0):\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    loss_list = []\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    for e in range(epochs):\n",
    "        print(f\"epoch: {e+1}/{epochs}\")\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            #x = x.unsqueeze(dim = 1)\n",
    "         \n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "        \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                loss_list.append(loss.item())\n",
    "                print(\"Train Accuracy:\")\n",
    "                train_acc = check_accuracy(loader_train, model)\n",
    "                print(\"Validation Accuracy:\")\n",
    "                val_acc = check_accuracy(loader_val, model)\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "                np.savetxt(\"train.txt\", np.array(train_acc_list))\n",
    "                np.savetxt(\"val_acc.txt\", np.array(val_acc_list))\n",
    "                np.savetxt(\"losslist.txt\", np.array(loss_list))\n",
    "                print()\n",
    "\n",
    "        if e % 20 == 0:\n",
    "            torch.save(model.state_dict(), f'model_weights_orig_conv_{e}.pth')\n",
    "            print(\"model saved\")\n",
    "                \n",
    "\n",
    "    return loss_list, train_acc_list, val_acc_list\n",
    "            \n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # x = x.unsqueeze(dim = 1)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            preds = torch.argmax(scores,dim=1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "\n",
    "def check_accuracy_conv(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.unsqueeze(dim = 1)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            preds = torch.argmax(scores,dim=1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logistic_regression = nn.Sequential(\n",
    "    nn.Linear(187541, 22),\n",
    ")\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = optim.SGD(logistic_regression.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "conv_model = nn.Sequential(\n",
    "    nn.Conv1d(1, 8, 9, padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(8,16,9, padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(16,32,9,  padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(32,32,9,  padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(32,32,9,  padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 2930, 22)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "loss_list, train_acc_list, val_acc_list = train(logistic_regression, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_checkpoint = torch.load('model_weights_orig_60.pth')\n",
    "conv_checkpoint = torch.load('model_weights_orig_conv.pth')\n",
    "\n",
    "logistic_regression.load_state_dict(logistic_checkpoint)\n",
    "logistic_regression.cuda()\n",
    "\n",
    "conv_model.load_state_dict(conv_checkpoint)\n",
    "conv_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency(seq, model):\n",
    "    #we don't need gradients w.r.t. weights for a trained model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    #set model in eval mode\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    #transoform input PIL image to torch.Tensor and normalize\n",
    "    # input = T(img)\n",
    "    # input.unsqueeze_(0)\n",
    "\n",
    "    #we want to calculate gradient of higest score w.r.t. input\n",
    "    #so set requires_grad to True for input \n",
    "    seq.to(device=device)  \n",
    "    seq.requires_grad = True\n",
    "    #forward pass to calculate predictions\n",
    "    preds = model(seq)\n",
    "    score = torch.max(preds)\n",
    "    #backward pass to get gradients of score predicted class w.r.t. input image\n",
    "    score.to(device=device)  \n",
    "\n",
    "    score.backward()\n",
    "    #get max along channel axis\n",
    "    saliency_map = torch.abs(seq.grad)\n",
    "    #normalize to [0..1]\n",
    "    saliency_map = (saliency_map - saliency_map.min())/(saliency_map.max()-saliency_map.min())\n",
    "\n",
    "    # #apply inverse transform on image\n",
    "    # with torch.no_grad():\n",
    "    #     input_img = seq[0]\n",
    "\n",
    "\n",
    "    input_vector = seq.cpu().detach().numpy()\n",
    "    saliency_map = saliency_map.cpu().detach().numpy()\n",
    "\n",
    "    # plt.clf()\n",
    "    saliency_map = np.reshape(saliency_map,(1,-1))\n",
    "    # plt.figure(figsize=(25,4))\n",
    "    # sns.heatmap(saliency_map,annot=True,cmap='viridis',cbar=True)\n",
    "    # plt.show()\n",
    "    return saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = x[2816]\n",
    "example_1 = example_1.cuda()\n",
    "\n",
    "example_2 = x[2763]\n",
    "example_2 = example_2.cuda()\n",
    "\n",
    "example_6 = x[2987]\n",
    "example_6 = example_6.cuda()\n",
    "\n",
    "print(y[2816])\n",
    "print(y[2763])\n",
    "print(y[2987])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_saliency_map = saliency(example_1, logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2_saliency_map = saliency(example_2, logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e3_saliency_map = saliency(example_6, logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_saliency_map = np.reshape(e1_saliency_map,(e1_saliency_map.shape[1],))\n",
    "e2_saliency_map = np.reshape(e2_saliency_map,(e2_saliency_map.shape[1],))\n",
    "e3_saliency_map = np.reshape(e3_saliency_map,(e3_saliency_map.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_argmax_idxs = np.argsort(e1_saliency_map)[-11:]\n",
    "print(f\"e1_argmax_idxs: {e1_argmax_idxs}, values: {e1_saliency_map[e1_argmax_idxs]}\")\n",
    "\n",
    "e2_argmax_idxs = np.argsort(e2_saliency_map)[-11:]\n",
    "print(f\"e2_argmax_idxs: {e2_argmax_idxs}, values: {e2_saliency_map[e2_argmax_idxs]}\")\n",
    "\n",
    "e3_argmax_idxs = np.argsort(e3_saliency_map)[-11:]\n",
    "print(f\"e3_argmax_idxs: {e3_argmax_idxs}, values: {e3_saliency_map[e3_argmax_idxs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(example_1.cpu().detach().numpy() - example_2.cpu().detach().numpy()))\n",
    "print(np.sum(example_2.cpu().detach().numpy() - example_6.cpu().detach().numpy()))\n",
    "print(np.sum(example_1.cpu().detach().numpy() - example_6.cpu().detach().numpy()))\n",
    "\n",
    "print(example_1[e1_argmax_idxs])\n",
    "print(example_2[e1_argmax_idxs])\n",
    "print(example_6[e1_argmax_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_labels = set()\n",
    "examples_list = []\n",
    "example_names = []\n",
    "\n",
    "counter = 0\n",
    "while len(seen_labels) < 22:\n",
    "    if labels[counter].item() not in seen_labels:\n",
    "        print(seen_labels)\n",
    "        example = x[counter]\n",
    "        example = example.cuda()\n",
    "        examples_list.append(x[counter])\n",
    "        seen_labels.add(labels[counter].item())\n",
    "        example_names.append(inverted_labels_dict[labels[counter].item()])\n",
    "    counter += 1\n",
    "\n",
    "indexes = []\n",
    "values = []\n",
    "for i,example in enumerate(examples_list):\n",
    "    print(f\"example {i} running\")\n",
    "    example = example.cuda()\n",
    "    cur_sal_map = saliency(example, logistic_regression)\n",
    "    cur_sal_map = np.reshape(cur_sal_map,(cur_sal_map.shape[1],))\n",
    "    cur_idxs = np.argsort(cur_sal_map)[-15:]\n",
    "    values.append(cur_sal_map[cur_idxs])\n",
    "    indexes.append(cur_idxs)\n",
    "\n",
    "\n",
    "\n",
    "for name, idx_list, value_list in zip(example_names,indexes,values):\n",
    "    print(f\"{name}: indices: {idx_list}, values: {value_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(loader_test, logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy_conv(loader_val, conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_analysis(loader, model):\n",
    "    model.eval()  \n",
    "    preds_list = []\n",
    "    y_list = []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        preds = torch.argmax(scores,dim=1)\n",
    "        preds_list += preds\n",
    "        y_list += y \n",
    "    return preds_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list, y_list = data_analysis(loader_test, logistic_regression)\n",
    "preds_list = [x.item() for x in preds_list]\n",
    "y_list = [x.item() for x in y_list]\n",
    "classes = set(y_list)\n",
    "totals_d = defaultdict(int)\n",
    "d = defaultdict(int)\n",
    "for pred, y in zip(preds_list,y_list):\n",
    "    totals_d[inverted_labels_dict[y]] += 1\n",
    "    if pred != y:\n",
    "        d[inverted_labels_dict[y]] += 1\n",
    "\n",
    "percent_d = defaultdict(float)\n",
    "for key,value in totals_d.items():\n",
    "    percent_d[key] = d[key]/value\n",
    "\n",
    "plt.bar(*zip(*percent_d.items()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(*zip(*totals_d.items()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "val_acc_list = np.loadtxt('val_acc.txt')\n",
    "train_acc_list = np.loadtxt('train.txt')\n",
    "loss_list = np.loadtxt('losslist.txt')\n",
    "\n",
    "print(len(val_acc_list))\n",
    "x_loss = [i for i in range(len(loss_list))]\n",
    "x_acc = [i for i in range(len(val_acc_list)/3)]\n",
    "\n",
    "x_acc = x_acc[1:]\n",
    "\n",
    "axs[0].plot(x_loss[2:], loss_list[2:])\n",
    "axs[0].set_title('Loss vs Iteration')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(x_acc, val_acc_list[1:], label='val')\n",
    "axs[1].plot(x_acc, train_acc_list[1:], label='train')\n",
    "axs[1].set_title('Accuracy vs Iteration')\n",
    "axs[1].set_xlabel('Iteration')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv(model, optimizer, epochs=1, start_epoch=0):\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    loss_list = []\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    for e in range(epochs):\n",
    "        print(f\"epoch: {e+1}/{epochs}\")\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x = x.unsqueeze(dim = 1)\n",
    "         \n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "        \n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                loss_list.append(loss.item())\n",
    "                print(\"Train Accuracy:\")\n",
    "                train_acc = check_accuracy_conv(loader_train, model)\n",
    "                print(\"Validation Accuracy:\")\n",
    "                val_acc = check_accuracy_conv(loader_val, model)\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "                np.savetxt(\"train.txt\", np.array(train_acc_list))\n",
    "                np.savetxt(\"val_acc.txt\", np.array(val_acc_list))\n",
    "                np.savetxt(\"losslist.txt\", np.array(loss_list))\n",
    "                torch.save(model.state_dict(), 'model_weights_orig_conv.pth')\n",
    "                print(\"model saved\")\n",
    "                print()\n",
    "                \n",
    "\n",
    "    return loss_list, train_acc_list, val_acc_list\n",
    "            \n",
    "def check_accuracy_conv(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.unsqueeze(dim = 1)\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            preds = torch.argmax(scores,dim=1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = nn.Sequential(\n",
    "    nn.Conv1d(1, 8, 9, padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(8,16,9, padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(16,32,9,  padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(32,32,9,  padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Conv1d(32,32,9,  padding = 'same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 2930, 22)\n",
    ")\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = optim.SGD(conv_model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Start training\n",
    "loss_list, train_acc_list, val_acc_list = train_conv(conv_model, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "236",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
