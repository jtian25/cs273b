{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss.\n",
    "print_every = 100\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyPickleDataset(Dataset):\n",
    "    def __init__(self, file_path, chunk_size=1000):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.total_length = self._get_total_length()\n",
    "        self.current_chunk = None\n",
    "        self.current_chunk_index = -1\n",
    "        \n",
    "    def _get_total_length(self):\n",
    "        with open(self.file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return len(data)\n",
    "    \n",
    "    def _load_chunk(self, chunk_index):\n",
    "        start = chunk_index * self.chunk_size\n",
    "        end = min(start + self.chunk_size, self.total_length)\n",
    "        with open(self.file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return data[start:end]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index < 0 or index >= self.total_length:\n",
    "            raise IndexError('Index out of range')\n",
    "        \n",
    "        chunk_index = index // self.chunk_size\n",
    "        if chunk_index != self.current_chunk_index:\n",
    "            self.current_chunk = self._load_chunk(chunk_index)\n",
    "            self.current_chunk_index = chunk_index\n",
    "        \n",
    "        item_index = index % self.chunk_size\n",
    "        return self.current_chunk[item_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LazyPickleDataset('x.pickle', chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('metadata_seurat_onelayer.csv')\n",
    "y = np.array(meta_data.loc[:,\"Celltype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set = set(y)\n",
    "print(len(y_set))\n",
    "labels_dict = defaultdict(int)\n",
    "label_num = 0\n",
    "for item in y_set:\n",
    "    labels_dict[item] = label_num\n",
    "    label_num += 1\n",
    "\n",
    "labels = []\n",
    "for item in y:\n",
    "    labels.append(labels_dict[item])\n",
    "labels = np.array(labels)\n",
    "labels = torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_DATAPOINTS = 14322\n",
    "NUM_TRAIN = 12890\n",
    "NUM_VAL = 716\n",
    "NUM_TEST = 716\n",
    "\n",
    "labels = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [NUM_TRAIN, NUM_TEST, NUM_VAL])\n",
    "\n",
    "loader_train = DataLoader(train_set, batch_size=64)\n",
    "loader_val = DataLoader(val_set, batch_size=64)\n",
    "loader_test = DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1, start_epoch=0):\n",
    "    model = model.to(device=device)\n",
    "    loss_list = []\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    for e in range(epochs):\n",
    "        print(f\"epoch: {e+1}/{epochs}\")\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x = x.unsqueeze(dim=1)\n",
    "            model.train()\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            print(\"iteration of data loader is \", t, \" with x of shape \" , x.shape(), \" and loss of \", loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                loss_list.append(loss.item())\n",
    "                print(\"Train Accuracy:\")\n",
    "                train_acc = check_accuracy(loader_train, model)\n",
    "                print(\"Validation Accuracy:\")\n",
    "                val_acc = check_accuracy(loader_val, model)\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "                np.savetxt(\"train.txt\", np.array(train_acc_list))\n",
    "                np.savetxt(\"val_acc.txt\", np.array(val_acc_list))\n",
    "                np.savetxt(\"losslist.txt\", np.array(loss_list))\n",
    "                print()\n",
    "\n",
    "    return loss_list, train_acc_list, val_acc_list\n",
    "            \n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.unsqueeze(dim=1)\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv1d(1, 128, 19),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Conv1d(128, 128, 19),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Conv1d(128, 256, 19),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Conv1d(256, 256, 19),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(2, stride=2),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1499456, 22)\n",
    ")\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "loss_list, train_acc_list, val_acc_list = train(model, optimizer, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = nn.Sequential(\n",
    "    nn.Linear(187541, 22),\n",
    ")\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = optim.SGD(logistic_regression.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "loss_list, train_acc_list, val_acc_list = train(logistic_regression, optimizer, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "val_acc_list = np.loadtxt('val_acc.txt')\n",
    "train_acc_list = np.loadtxt('train.txt')\n",
    "loss_list = np.loadtxt('losslist.txt')\n",
    "\n",
    "print(len(val_acc_list))\n",
    "x_loss = [i for i in range(len(loss_list))]\n",
    "x_acc = [i for i in range(len(val_acc_list)/3)]\n",
    "\n",
    "x_acc = x_acc[1:]\n",
    "\n",
    "axs[0].plot(x_loss[2:], loss_list[2:])\n",
    "axs[0].set_title('Loss vs Iteration')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(x_acc, val_acc_list[1:], label='val')\n",
    "axs[1].plot(x_acc, train_acc_list[1:], label='train')\n",
    "axs[1].set_title('Accuracy vs Iteration')\n",
    "axs[1].set_xlabel('Iteration')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs273b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
